<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Part 2: Ambient Data Logger for Raspberry Pi | John Heaven</title><meta name=keywords content><meta name=description content="This is part of a series on my ambient data logging and visualisation setup.
I&rsquo;ve already given an overview of the entire architecture, and written about setting up the Raspberry Pico(s) as sensor devices. Now I&rsquo;m going to explain how I collect that data in a CSV and Postgres database.
Dumb sensors The sensors themselves are designed to be quite dumb. They don&rsquo;t log data locally because it&rsquo;s easier to access it if it&rsquo;s stored centrally."><meta name=author content="John Heaven"><link rel=canonical href=/post/ambient-data-logger-for-rpi/><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Part 2: Ambient Data Logger for Raspberry Pi"><meta property="og:description" content="This is part of a series on my ambient data logging and visualisation setup.
I&rsquo;ve already given an overview of the entire architecture, and written about setting up the Raspberry Pico(s) as sensor devices. Now I&rsquo;m going to explain how I collect that data in a CSV and Postgres database.
Dumb sensors The sensors themselves are designed to be quite dumb. They don&rsquo;t log data locally because it&rsquo;s easier to access it if it&rsquo;s stored centrally."><meta property="og:type" content="article"><meta property="og:url" content="/post/ambient-data-logger-for-rpi/"><meta property="og:image" content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-11-23T11:39:35+01:00"><meta property="article:modified_time" content="2022-11-23T11:39:35+01:00"><meta property="og:site_name" content="John Heaven"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Part 2: Ambient Data Logger for Raspberry Pi"><meta name=twitter:description content="This is part of a series on my ambient data logging and visualisation setup.
I&rsquo;ve already given an overview of the entire architecture, and written about setting up the Raspberry Pico(s) as sensor devices. Now I&rsquo;m going to explain how I collect that data in a CSV and Postgres database.
Dumb sensors The sensors themselves are designed to be quite dumb. They don&rsquo;t log data locally because it&rsquo;s easier to access it if it&rsquo;s stored centrally."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"/post/"},{"@type":"ListItem","position":3,"name":"Part 2: Ambient Data Logger for Raspberry Pi","item":"/post/ambient-data-logger-for-rpi/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Part 2: Ambient Data Logger for Raspberry Pi","name":"Part 2: Ambient Data Logger for Raspberry Pi","description":"This is part of a series on my ambient data logging and visualisation setup.\nI\u0026rsquo;ve already given an overview of the entire architecture, and written about setting up the Raspberry Pico(s) as sensor devices. Now I\u0026rsquo;m going to explain how I collect that data in a CSV and Postgres database.\nDumb sensors The sensors themselves are designed to be quite dumb. They don\u0026rsquo;t log data locally because it\u0026rsquo;s easier to access it if it\u0026rsquo;s stored centrally.","keywords":[],"articleBody":"This is part of a series on my ambient data logging and visualisation setup.\nI’ve already given an overview of the entire architecture, and written about setting up the Raspberry Pico(s) as sensor devices. Now I’m going to explain how I collect that data in a CSV and Postgres database.\nDumb sensors The sensors themselves are designed to be quite dumb. They don’t log data locally because it’s easier to access it if it’s stored centrally. They don’t proactively post data, because it’s more convenient to determine the frequency of readings centrally. You don’t want to be syncronising data or changing frequency or data logging by downloading and uploading data to each of your sensors.\nThe downside is that if data doesn’t get collected, it doesn’t exist. This is why we need this part of the system: it needs to query the sensors regularly and record the response. A potential disadvantage is that if the connection is broken, then data gets lost.\nDocker The code is designed to be run with Docker (or potentially another container solution). This is why (at the moment) there is no way to specify, e.g. as a command-line argument, where to store the CSV file, or where to get your settings from. You simply mount the folder where these files exist when running the container.\nThere is, however, a script in the respository for running the container. You can specify the data directory and the cache directory (more on that later) as arguments.\nNo timer included This script only takes one reading from each of the sensors, logs them, then it’s done. So the way to take regular readings is to use a cron job or systemd service to fire up the container as often as you want.\nMy cron looks like this:\n*/10 * * * * /home/johnheaven/repos/ambient-data-logger/docker-run.sh /mnt/general-files/ambient-data /home/johnheaven/data/ambient-data-logger/cache\nIt runs the docker container every ten minutes, then the data gets logged. /mnt/general-files/ambient-data is where the CSV with all my data is, and /home/johnheaven/data/ambient-data-logger/cache is the cache for the IP address, which I’ll explain later.\nLogging to CSV As explained above, the data gets logged to CSV. This is always within the data directory for the reasons mentioned above. The headings don’t get written to the CSV file (as it’s a once-in-a-lifetime operation), so if you want them you have to (carefully) add them before or after generating the first readings.\nThe headings are:\ntime,temp,pressure,humidity,sensor,pico_id,pico_uuid\nLogging to Postgres Additionally, data gets logged to Postgres (or, with modification, any other database). It just has to be available through the data you specify in environment variables (or a .env) file. They should be fairly explanatory and here is an example:\nDB_DRIVER=postgresql DB_USER=jheaven DB_PASS=[your_password_here] DB_HOST=postgres DB_PORT=5432 DB_NAME=jheaven If you don’t know what a .env file is: it is a way of passing variables to Python that get overridden by environment variables if they exist. In our case, assuming we’re using Docker, these environment variables will only exist if they are passed into the run command with the -e option. You could do it that way, too, if you modify the run-docker.sh script.\nSo what does the code do and how do I set it up? Okay, so we’ve got a rough idea of how this thing all fits together. Here’s how to install it, step by step. I’m going to assume you have a Raspberry Pi capable of running Docker (i.e. not running Raspbian but something like Manjaro or Ubuntu Server instead).\nInstall docker sudo pacman -S docker or apt-get install docker Get the repo and put it somewhere sensible git clone https://github.com/johnheaven/ambient-data-logger.git Set up a Postgres database (preferably with Docker) -\u003e docker run --mount type=bind,dst=/var/lib/postgresql/data -d -e POSTGRES_USER=[pgusername] -e POSTGRES_PASSWORD=[pgpass] -e POSTGRES_DB=[dbname] -p 8080:8080 --restart always --name postgres postgres:15-alpine You will need to replace [pgusername], [pgpass] and [dbname] with appropriate values and include them in the .env file This will be available on your local network too, so bear that in mind if it poses a security issue for you Create a .env file as above, and insert the values you just used to create your Postgres database In the src directory, copy example.settings.py to settings.py and insert the name of each of your sensors as a string in the list. You should have assigned these while setting up your Pico(s) Build the Docker image by changing into the directory where the git project has been cloned into, then typing docker build . --name ambient-data-logger Set up your cron job with crontab -e, e.g. */10 * * * * [location of rep]/ambient-data-logger/docker-run.sh [folder where CSV should be stored] [folder where cache can be stored] It should work now. Fingers crossed… What the code does There’s not a whole lot going on in the code. It just wants to know which sensors to query, then it queries them with the requests library, logs the data, and it’s done.\nIt uses sqlalchemy for the SQL part, which is why you might get away with using a database other than Postgres.\nThere’s one catch, which makes things a little complicated (or it did for me, at least). The script needs to know where on the network to find the Picos. But the Picos can’t identify themselves: it’s impossible to set a custom hostname. Normally, a fixed IP would do the trick, but my router doesn’t allow that. So I came up with a hacky solution, which is the ip_search object.\nIt starts from a particular IP, and tries to contact a device. If that doesn’t work, it tries a fixed number of IPs before giving up. It also checks whether the ID is correct (which is something I need to optimise), and only logs data if it’s the right one.\nTo save it doing this search every time, it caches the best IP in a JSON file. So next time, it starts from there and this is almost always the right IP at least for a few weeks. If it doesn’t work on your router, e.g. because your network allocates IPs in a different range, you could try modifying the following part of the code:\nips = ip_search(starting_ip_last_3=starting_ip_last_3, max_steps=10) This is where the ip_search object is initiated. There’s a default option ip_template='http://192.168.2.%s/data/'. It only changes the last 3 numbers in the IP. You can modify the bit before that to match your network setup. (This is rather crude, I know.)\nYou can also change how far it searches: by default, it only searches 20 steps (10 steps ‘outwards’ in either direction). That’s the max_steps parameter.\nFinally, the place it starts might be wrong. For the very first time it starts, it’s hard-coded to start at 145 (i.e. 192.168.2.145) and search from there. That’s the starting_ip_last_3 = 145 statement in ambient-data-logger.py. You could try changing that and seeing how you get along. Maxing out the steps might be the easiest way, because the next time it will look up the best value from the cache.\nAnd that’s it. The only step left in the process is to start visualising the data with Grafana.\n","wordCount":"1174","inLanguage":"en","datePublished":"2022-11-23T11:39:35+01:00","dateModified":"2022-11-23T11:39:35+01:00","author":{"@type":"Person","name":"John Heaven"},"mainEntityOfPage":{"@type":"WebPage","@id":"/post/ambient-data-logger-for-rpi/"},"publisher":{"@type":"Organization","name":"John Heaven","logo":{"@type":"ImageObject","url":"%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href accesskey=h title="John Heaven (Alt + H)"><img src=/apple-touch-icon.png alt aria-label=logo height=35>John Heaven</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=/about/ title=About><span>About</span></a></li><li><a href=/post/ title=Posts><span>Posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href>Home</a>&nbsp;»&nbsp;<a href=/post/>Posts</a></div><h1 class=post-title>Part 2: Ambient Data Logger for Raspberry Pi</h1><div class=post-meta><span title='2022-11-23 11:39:35 +0100 CET'>November 23, 2022</span>&nbsp;·&nbsp;1174 words&nbsp;·&nbsp;John Heaven</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#dumb-sensors>Dumb sensors</a></li><li><a href=#docker>Docker</a></li></ul><ul><li><a href=#logging-to-csv>Logging to CSV</a></li><li><a href=#logging-to-postgres>Logging to Postgres</a></li><li><a href=#so-what-does-the-code-do-and-how-do-i-set-it-up>So what does the code do and how do I set it up?</a><ul><li><a href=#what-the-code-does>What the code does</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>This is part of a series on my ambient data logging and visualisation setup.</p><p>I&rsquo;ve already given an overview of the entire architecture, and written about setting up the Raspberry Pico(s) as sensor devices. Now I&rsquo;m going to explain how I collect that data in a CSV and Postgres database.</p><h2 id=dumb-sensors>Dumb sensors<a hidden class=anchor aria-hidden=true href=#dumb-sensors>#</a></h2><p>The sensors themselves are designed to be quite dumb. They don&rsquo;t log data locally because it&rsquo;s easier to access it if it&rsquo;s stored centrally. They don&rsquo;t proactively post data, because it&rsquo;s more convenient to determine the frequency of readings centrally. You don&rsquo;t want to be syncronising data or changing frequency or data logging by downloading and uploading data to each of your sensors.</p><p>The downside is that if data doesn&rsquo;t get collected, it doesn&rsquo;t exist. This is why we need this part of the system: it needs to query the sensors regularly and record the response. A potential disadvantage is that if the connection is broken, then data gets lost.</p><h2 id=docker>Docker<a hidden class=anchor aria-hidden=true href=#docker>#</a></h2><p>The code is designed to be run with Docker (or potentially another container solution). This is why (at the moment) there is no way to specify, e.g. as a command-line argument, where to store the CSV file, or where to get your settings from. You simply mount the folder where these files exist when running the container.</p><p>There is, however, a script in the respository for running the container. You can specify the data directory and the cache directory (more on that later) as arguments.</p><h1 id=no-timer-included>No timer included<a hidden class=anchor aria-hidden=true href=#no-timer-included>#</a></h1><p>This script only takes one reading from each of the sensors, logs them, then it&rsquo;s done. So the way to take regular readings is to use a cron job or systemd service to fire up the container as often as you want.</p><p>My cron looks like this:</p><p><code>*/10 * * * * /home/johnheaven/repos/ambient-data-logger/docker-run.sh /mnt/general-files/ambient-data /home/johnheaven/data/ambient-data-logger/cache</code></p><p>It runs the docker container every ten minutes, then the data gets logged. <code>/mnt/general-files/ambient-data</code> is where the CSV with all my data is, and <code>/home/johnheaven/data/ambient-data-logger/cache</code> is the cache for the IP address, which I&rsquo;ll explain later.</p><h2 id=logging-to-csv>Logging to CSV<a hidden class=anchor aria-hidden=true href=#logging-to-csv>#</a></h2><p>As explained above, the data gets logged to CSV. This is always within the data directory for the reasons mentioned above. The headings don&rsquo;t get written to the CSV file (as it&rsquo;s a once-in-a-lifetime operation), so if you want them you have to (carefully) add them before or after generating the first readings.</p><p>The headings are:</p><p><code>time,temp,pressure,humidity,sensor,pico_id,pico_uuid</code></p><h2 id=logging-to-postgres>Logging to Postgres<a hidden class=anchor aria-hidden=true href=#logging-to-postgres>#</a></h2><p>Additionally, data gets logged to Postgres (or, with modification, any other database). It just has to be available through the data you specify in environment variables (or a <code>.env</code>) file. They should be fairly explanatory and here is an example:</p><pre tabindex=0><code>DB_DRIVER=postgresql
DB_USER=jheaven
DB_PASS=[your_password_here]
DB_HOST=postgres
DB_PORT=5432
DB_NAME=jheaven
</code></pre><p>If you don&rsquo;t know what a <code>.env</code> file is: it is a way of passing variables to Python that get overridden by environment variables if they exist. In our case, assuming we&rsquo;re using Docker, these environment variables will only exist if they are passed into the <code>run</code> command with the <code>-e</code> option. You could do it that way, too, if you modify the <code>run-docker.sh</code> script.</p><h2 id=so-what-does-the-code-do-and-how-do-i-set-it-up>So what does the code do and how do I set it up?<a hidden class=anchor aria-hidden=true href=#so-what-does-the-code-do-and-how-do-i-set-it-up>#</a></h2><p>Okay, so we&rsquo;ve got a rough idea of how this thing all fits together. Here&rsquo;s how to install it, step by step. I&rsquo;m going to assume you have a Raspberry Pi capable of running Docker (i.e. not running Raspbian but something like Manjaro or Ubuntu Server instead).</p><ul><li>Install docker <code>sudo pacman -S docker</code> or <code>apt-get install docker</code></li><li>Get the repo and put it somewhere sensible <code>git clone https://github.com/johnheaven/ambient-data-logger.git</code></li><li>Set up a Postgres database (preferably with Docker) -> <code>docker run --mount type=bind,dst=/var/lib/postgresql/data -d -e POSTGRES_USER=[pgusername] -e POSTGRES_PASSWORD=[pgpass] -e POSTGRES_DB=[dbname] -p 8080:8080 --restart always --name postgres postgres:15-alpine</code><ul><li>You will need to replace [pgusername], [pgpass] and [dbname] with appropriate values and include them in the <code>.env</code> file</li><li>This will be available on your local network too, so bear that in mind if it poses a security issue for you</li></ul></li><li>Create a <code>.env</code> file as above, and insert the values you just used to create your Postgres database</li><li>In the <code>src</code> directory, copy <code>example.settings.py</code> to <code>settings.py</code> and insert the name of each of your sensors as a string in the list. You should have assigned these while setting up your Pico(s)</li><li>Build the Docker image by changing into the directory where the git project has been cloned into, then typing <code>docker build . --name ambient-data-logger</code></li><li>Set up your cron job with <code>crontab -e</code>, e.g. <code>*/10 * * * * [location of rep]/ambient-data-logger/docker-run.sh [folder where CSV should be stored] [folder where cache can be stored]</code></li><li>It should work now. Fingers crossed&mldr;</li></ul><h3 id=what-the-code-does>What the code does<a hidden class=anchor aria-hidden=true href=#what-the-code-does>#</a></h3><p>There&rsquo;s not a whole lot going on in the code. It just wants to know which sensors to query, then it queries them with the <code>requests</code> library, logs the data, and it&rsquo;s done.</p><p>It uses sqlalchemy for the SQL part, which is why you might get away with using a database other than Postgres.</p><p>There&rsquo;s one catch, which makes things a little complicated (or it did for me, at least). The script needs to know where on the network to find the Picos. But the Picos can&rsquo;t identify themselves: it&rsquo;s impossible to set a custom hostname. Normally, a fixed IP would do the trick, but my router doesn&rsquo;t allow that. So I came up with a hacky solution, which is the <code>ip_search</code> object.</p><p>It starts from a particular IP, and tries to contact a device. If that doesn&rsquo;t work, it tries a fixed number of IPs before giving up. It also checks whether the ID is correct (which is something I need to optimise), and only logs data if it&rsquo;s the right one.</p><p>To save it doing this search every time, it caches the best IP in a JSON file. So next time, it starts from there and this is almost always the right IP at least for a few weeks. If it doesn&rsquo;t work on your router, e.g. because your network allocates IPs in a different range, you could try modifying the following part of the code:</p><pre tabindex=0><code> ips = ip_search(starting_ip_last_3=starting_ip_last_3, max_steps=10)
</code></pre><p>This is where the <code>ip_search</code> object is initiated. There&rsquo;s a default option <code>ip_template='http://192.168.2.%s/data/'</code>. It only changes the last 3 numbers in the IP. You can modify the bit before that to match your network setup. (This is rather crude, I know.)</p><p>You can also change how far it searches: by default, it only searches 20 steps (10 steps &lsquo;outwards&rsquo; in either direction). That&rsquo;s the <code>max_steps</code> parameter.</p><p>Finally, the place it starts might be wrong. For the very first time it starts, it&rsquo;s hard-coded to start at 145 (i.e. 192.168.2.145) and search from there. That&rsquo;s the <code>starting_ip_last_3 = 145</code> statement in <code>ambient-data-logger.py</code>. You could try changing that and seeing how you get along. Maxing out the steps might be the easiest way, because the next time it will look up the best value from the cache.</p><p>And that&rsquo;s it. The only step left in the process is to start visualising the data with Grafana.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=/post/ambient-data-api-with-raspberry-pico/><span class=title>Next »</span><br><span>Part 1: Building an Ambient Data API With the Raspberry Pico W</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2022 <a href>John Heaven</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>